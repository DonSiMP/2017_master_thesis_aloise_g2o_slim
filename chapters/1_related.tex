\chapter{Related Works}\label{ch:related}
\lettrine[lines=2]{S}{}imultaneous Localization and Mapping represents a well known complex mathematical problem, based on non-linear optimization. It has been studied by the robotics community since the 80s \cite{durrant2006simultaneous} \cite{bailey2006simultaneous}; during this early stage, its statistical formulation has been investigated, proposing interesting results that will constitute the baseline basically for all the future SLAM systems.

After some years, in the 90s, early solutions to the SLAM problem start to arise. The first systems able to produce appreciable results in terms of speed and accuracy were based on \textit{Extended Kalman Filters} (EKF) \cite{leonard1990dynamic} \cite{dissanayake2001solution}. EKFs allow to deal with problem's non-linearity through effective approximations and to represent multivariate distributions with a small number of parameters. This success encouraged the research community to perform deeper investigations in \textit{filtering} approaches \cite{aulinas2008filtering_review}. \textit{Particle filters} started to gain popularity, in particular \textit{Rao-Blackwellized Particle Filters} \cite{grisetti2005improving}: the work of Montemerlo \textit{et al.} \cite{montemerlo2002fastslam} was the first SLAM system able to deal with thousand of landmarks with a good accuracy. 

However, \textit{filtering} approaches revealed to be not the best answer to the SLAM problem due to the computational complexity of the solution, especially when dimensions grow. Moreover, system's accuracy is affected by the non-linearities, leading to non-optimal solutions. \textit{Maximum A Posteriori} (MAP) starts to be taken in consideration and the community has taken a step back to the work of Lu \textit{et al.} \cite{lu1997globally}. Filtering-based approaches align local pose frames incrementally and, thus, different parts of the model are updated independently, generating inconsistencies in the model. MAP optimization takes in consideration all the local frames and the relations between them at once, leading to a more consistent model and to better accuracy. Lu \textit{et al.} embedded all the pose relations into an network with nodes and edges, allowing efficient optimization. However, at that time the computational power was not enough to deliver good performance and, thus, this solution was put aside.

Nevertheless, this work represents the precursor to one the most intuitive SLAM formulation, called \textit{graph-based SLAM}, that exploits the computing power of recent robots to deliver impressive performances. In this paradigm, the robot builds an \textit{hyper-graph} whose nodes represents either robot poses or salient points in the world - called \textit{landmarks} - while the hyper-edges encode sensors' measurements between subsets of nodes. 

Graph-based SLAM systems have two main components: \textit{front-end} and \textit{back-end}. The former one uses data acquired by robot's sensors to populate the hyper-graph, abstracting raw data into a model that is amenable for optimization. The front-end has to determine the most likely constraint that involves a subset of nodes given an incoming measurement, solving the so called \textit{data association} problem. This includes \textit{short-term data association} and \textit{long-term} one. The former has to match corresponding extracted features with consecutive sensor measurements - e.g. stating that visual features detected in multiple consecutive frames represent the same 3D world point. The latter one, instead, expresses a more complex problem: it has to associate new measurements to already encountered world points, generating the so called \textit{loop-closures} - e.g. when a robot passes multiple times in a place, it has to recognize that it is re-observing the same points in order to generate a map that is consistent with the environment. As for the sensors used, state-of-the-art systems usually acquire data from cameras (RGB or RDB-D) or 3D-LiDARs. The former, in particular, it is gaining much attention since they are - generally - cheap and can be mounted basically on every electronic device in single or stereo configurations. 

This work, however, focuses on system's back-end, assuming that the given front-end provides consistent estimates. The back-end takes as input the graph and computes the most-likely map given all the constraints. Systems based on this formulation represent the gold standard for map optimization, thus, in the next sections it is proposed a brief overview of the most successful implementations.

\section{Dense Approaches}
The work of Lu \textit{et al.} \cite{lu1997globally} was the first of its kind: map estimation is obtained through global optimization of the error function deriving from constraints between different poses. They employed a combination of \textit{relation-based} and \textit{location-based} representations, where the former were fixed while the latter were treated as free variables. Those pose-relations were used to construct a network whose nodes were robot poses taken from its trajectory while the constraints between nodes were the pose-relations. Finally, the optimization problem exploits the network to obtain an objective function that will be minimized: the total energy will decrease as the difference between estimated relative-pose that involves two nodes and the measured value tends to zero. 

It is good to notice that, in this formulation, the computational power needed for the optimization grow \textit{cubically} with the number of variables involved - graph's nodes. Gutmann and Konolige addressed this problem in their work \cite{gutmann1999incremental} proposing a method to incrementally build the network and that determines topologically correct relations between poses.

Those approaches opened the path to a series of study in this direction, that will lead to current state-of-the-art optimizers.

\section{Olson's Gradient Descent}
Evident limitations of the previously seen approaches is that their solution highly depends from the initial estimate of the state. The initial guess is derived from dead-reckoning and, thus, if this is not good the system will converge to a local minimum, giving a sub-optimal solution. Olson \textit{et al.} proposed in their work \cite{olson2006fast} addressed this issue, proposing a non-linear optimization algorithm that quickly converges to a good approximation of the global minimum. 

They achieved such results combining two new aspects: the first one is the use of a variant of \textit{Stochastic Gradient Descent} algorithm, which is robust against local minima and has a fast convergence rate; the second one is an \textit{alternative state-space representation} that has good stability and computational properties. The latter, in particular, allows to update many poses with a relatively small computational cost in a single iteration. Moreover, the memory consumption has been lowered together with the run time - respectively $O(N + M)$ and $O(\log(N))$, where $N$ represents the number of poses and $M$ the number of constraints.

\section{Smoothing and Mapping}
Smoothing and Mapping, shortened as \textit{SAM}, follows the path of global trajectory optimization described in the previous Section. Dellaert \textit{et al.} proposed with \textit{square root SAM} ($\sqrt{SAM}$) \cite{dellaert2006square} a system able to deal with \textit{full SLAM problems}, which involves estimating the entire set of sensor poses along with the parameters of all the features in the environment - also known in photogrammetry as \textit{bundle adjustment} and as \textit{structure from motion} in computer vision.

$\sqrt{SAM}$ performs fast optimization exploiting the problem's intrinsic sparsity. Knowing that the measurement Jacobian matrix $A$ is sparse, it is possible to solve the relative linear system in a faster way through a good \textit{variable reordering} together with \textit{QR} or \textit{Cholesky} factorization. For those reasons, $\sqrt{SAM}$ can optimize larger graph without losing in terms of performances or accuracy.

Further improvements were introduced with \textit{iSAM} - \textit{incremental} SAM - developed by Kaess \textit{et al.} \cite{kaess2007isam}. The foundations were the same as $\sqrt{SAM}$, but in this case the system operates incrementally, without the need of fully refactoring the whole QR-decomposition, but updating it every time a new measurement is available. With this solution, it was possible to address real-time problems since the optimization process is faster than before. 

The next iteration of this branch of solutions, is represented by \textit{iSAM2} \cite{kaess2012isam2}. In this case, a new data-structure is proposed, the \textit{Bayes tree}, to map better the square root information matrix of the problem. Employing Bayes trees, the algorithm is able to further improve the performances, exploiting incremental variable re-ordering and fluid relinearization, eliminating the need for periodic batch steps. 

\section{TORO}
Grisetti \textit{et al.} proposed in their work TORO \cite{toro} - Tree-based netwORk Optimizer - an extension of Olson's algorithm to efficiently manage 2D and 3D graph-based optimization problems. 

This frameworks has several features that allow it to achieve good performances in terms of speed and accuracy. The first one is a revisited version of the standard \textit{Stochastic Gradient Descent} used to perform the optimization process, together with a technique to efficiently distribute the \textit{rotational error} over a sequence of 3D poses \cite{grisetti2007efficient}. In fact, due to the non-commutativity of rotational angles in 3D, major problem may arise when applying approaches that are designed for a 2D world. As a result, TORO converges by orders of magnitude faster with respect to previous approaches.

Moreover, it employs a \textit{tree parametrization of the nodes} \cite{grisetti2007tree} that significantly improves the performances and allows to deal with arbitrary network topologies. This consented the authors to bound algorithm complexity to the size of the mapped area and not to the trajectory's length, yielding accurate maps of the environment in a small amount of time.

\section{G2O}
The work of K\"ummerle \textit{et al.} \cite{kummerle2011g} is an open-source C++ frameworks for optimizing \textit{non-linear least squares problems} that can be represented as a graph. Its generality together with a cross-platform implementation made $g^2o$ one of the most successful graph optimization tool, which is employed in many state-of-the-art full SLAM systems. 

This works focuses on efficiency, which is achieved at various levels: it exploits graph sparsity and takes advantage of the graph's special structure to perform fast optimization; it uses advanced methods - Cholesky decomposition through CHOLMOD library - to solve sparse linear system; finally, it utilizes modern processor's features to perform fast math operation optimizing cache usage - e.g. SIMD instructions. Moreover, this frameworks offers the possibility to choose between different algorithms - i.e. Gauss-Newton, Levenberg-Marquardt - and linear solvers - direct and iterative.

Our approach still delivers comparable performances with respect to $g^2o$ while being lightweight and more simple to include in a full SLAM pipeline. In fact, $g^2o$ is a big framework, with over 40 thousands lines of code and, thus, its inclusion may add weight to the system.

\section{GT-SAM}
GT-SAM is a C++ library developed by Dellaert \textit{et al.} \cite{dellaert2012gtsam} at the Georgia Institute of Technology, which provides solution to a wide range of SLAM and Structure From Motion (SFM) problems, based on factor-graph optimization. It provides both C++ and MATLAB implementations that allow to easily develop and visualize problem solutions. 

This works - like it has been previously seen in $g^2o$ - also focuses on efficient optimization and takes advantage of the graph sparsity to deliver fast and accurate performances. However, this framework is even more big and complex - over 300 thousands lines of code - making it very difficult to unravel and understand what is under the hood.

\section{HOG-Man}
Grisetti \textit{et al.} in their work \cite{grisetti2010hogman} proposed an optimization system designed for on-line operations and that is accurate, fast and memory efficient: \textit{HOG-Man} - which stands for Hierarchical Optimization on Manifolds.

At its core there is a \textit{hierarchical} approach to graph optimization: during on-line mapping, it optimizes only the coarse structure of the environment and not the whole map.

\section{Condensed Measurements}
\lipsum[1-2]

\section{Tectonic-SAM}
\lipsum[1-2]
