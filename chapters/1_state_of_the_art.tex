\chapter{State of the Art}\label{ch:related}
\lettrine[lines=2]{S}{}imultaneous Localization and Mapping represents a well known complex mathematical problem, based on non-linear optimization. It has been studied by the robotics community since the 80s \cite{durrant2006simultaneous} \cite{bailey2006simultaneous}; during this early stage, its statistical formulation has been investigated, proposing interesting results that will constitute the baseline basically for all the future SLAM systems.

After some years, in the 90s, early solutions to the SLAM problem start to arise. The first systems able to produce appreciable results in terms of speed and accuracy were based on \textit{Extended Kalman Filters} (EKF) \cite{leonard1990dynamic} \cite{dissanayake2001solution}. EKFs allow to deal with problem's non-linearity through effective approximations and to represent multivariate distributions with a small number of parameters. This success encouraged the research community to perform deeper investigations in \textit{filtering} approaches \cite{aulinas2008filtering_review}. \textit{Particle filters} started to gain popularity, in particular \textit{Rao-Blackwellized Particle Filters} \cite{grisetti2005improving}: the work of Montemerlo \textit{et al.} \cite{montemerlo2002fastslam} was the first SLAM system able to deal with thousand of landmarks with a good accuracy. 

However, \textit{filtering} approaches revealed to be not the best answer to the SLAM problem due to the computational complexity of the solution, especially when dimensions grow. Moreover, system's accuracy is affected by the non-linearities, leading to non-optimal solutions. \textit{Maximum A Posteriori} (MAP) starts to be taken in consideration and the community has taken a step back to the work of Lu \textit{et al.} \cite{lu1997globally}. Filtering-based approaches align local pose frames incrementally and, thus, different parts of the model are updated independently, generating inconsistencies in the model. MAP optimization takes in consideration all the local frames and the relations between them at once, leading to a more consistent model and to better accuracy. Lu \textit{et al.} embedded all the pose relations into an network with nodes and edges, allowing efficient optimization. However, at that time the computational power was not enough to deliver good performance and, thus, this solution was put aside.

Nevertheless, this work represents the precursor to one the most intuitive SLAM formulation, called \textit{graph-based SLAM}, that exploits the computing power of recent robots to deliver impressive performances. In this paradigm, the robot builds an \textit{hyper-graph} whose nodes represents either robot poses or salient points in the world - called \textit{landmarks} - while the hyper-edges encode sensors' measurements between subsets of nodes. 

Graph-based SLAM systems have two main components: \textit{front-end} and \textit{back-end}. The former one uses data acquired by robot's sensors to populate the hyper-graph, abstracting raw data into a model that is amenable for optimization. The front-end has to determine the most likely constraint that involves a subset of nodes given an incoming measurement, solving the so called \textit{data association} problem. This includes \textit{short-term data association} and \textit{long-term} one. The former has to match corresponding extracted features with consecutive sensor measurements - e.g. stating that visual features detected in multiple consecutive frames represent the same 3D world point. The latter one, instead, expresses a more complex problem: it has to associate new measurements to already encountered world points, generating the so called \textit{loop-closures} - e.g. when a robot passes multiple times in a place, it has to recognize that it is re-observing the same points in order to generate a map that is consistent with the environment. As for the sensors used, state-of-the-art systems usually acquire data from cameras (RGB or RDB-D) or 3D-LiDARs. The former, in particular, it is gaining much attention since they are - generally - cheap and can be mounted basically on every electronic device in single or stereo configurations. Current benchmark systems for \textit{monocular visual SLAM} are ORB-SLAM \cite{mur2015orb-slam} and LSD-SLAM \cite{engel2014lsd-slam} while for stereo configurations it is impossible to ignore the freshly released ORB-SLAM2 \cite{mur2017orb-slam2}. Both ORB-SLAM and ORB-SLAM2 rely on the extraction of visual features from the scene, while LSD-SLAM uses raw image data to track robot motion, making it more robust to environmental changes at the cost of more computational power needed. 

This work, however, focuses on system's back-end, assuming that the given front-end provides consistent estimates. The back-end takes as input the graph and computes the most-likely map given all the constraints. Systems based on this formulation represent the gold standard for map optimization, thus, in the next sections it is proposed a brief overview of the most successful implementations.

\section{G2O}
The work of K\"ummerle \textit{et al.} \cite{kummerle2011g} is an open-source C++ frameworks for optimizing \textit{non-linear least squares problems} that can be represented as a graph. Its generality together with a cross-platform implementation made g2o one of the most successful graph optimization tool, which is employed in many state-of-the-art full SLAM systems. 

This works focuses on efficiency, which is achieved at various levels: it exploits graph sparsity and takes advantage of the graph's special structure to perform fast optimization; it uses advanced methods - Cholesky decomposition through CHOLMOD library - to solve sparse linear system; finally, it utilizes modern processor's features to perform fast math operation optimizing cache usage - e.g. SIMD instructions. Moreover, this frameworks offers the possibility to choose between different algorithms - i.e. Gauss-Newton, Levenberg-Marquardt - and linear solvers - direct and iterative.

Our approach still delivers comparable performances with respect to $g^2o$ while being lightweight and more simple to include in a full SLAM pipeline. In fact, $g^2o$ is a big framework, with over 40 thousands lines of code and, thus, its inclusion may add weight to the system.

\section{GT-SAM}
GT-SAM is a C++ library developed by Dellaert \textit{et al.} \cite{dellaert2012gtsam} at the Georgia Institute of Technology, which provides solution to a wide range of SLAM and Structure From Motion (SFM) problems, based on factor-graph optimization. It provides both C++ and MATLAB implementations that allow to easily develop and visualize problem solutions. 

This works - like it has been previously seen in $g^2o$ - also focuses on efficient optimization and takes advantage of the graph sparsity to deliver fast and accurate performances. However, this framework is even more big and complex - over 300 thousands lines of code - making it very difficult to unravel and understand what is under the hood.