\chapter{Solving Factor Graphs with SE3 Variables}\label{ch:solvingSE3}

In this project it has been mainly addressed the optimization of \textbf{3D factor graphs}. In particular it has been developed from scratch a back-end system to efficiently solve \textit{pose-graph optimization} and \textit{bundle adjustment}.

The reader might feel already comfortable with the formulation of those problems, thus, in this Section will be shown more in detail the approaches used in order to achieve \textbf{real-time performances} and how $SE(3)$ constraints have been manipulated in order to \textbf{reduce problem's non-linearity}. 

\section{Exploit Sparsity}\label{sec:sparsity}
As it as been already mentioned in Chapter \ref{ch:basics}, standard optimization algorithms like \textit{Gauss-Newton} or \textit{Levenberg-Marquardt} reduce the non-linear problem to the solution of a \textit{linear system}, namely

\begin{equation*}
    \hessian \dx = -\bvec
\end{equation*}

\noindent Here, $\dx$ and $\bvec$ are dense vectors, while the \textit{Hessian}'s approximation $\hessian$ is a sparse matrix. The literature proposes a lot of methods to solve efficiently sparse linear systems. Those can be categorized in 2 main groups:

\begin{enumerate}
    \item \textbf{Iterative methods} \cite{saad2003iterativeSPsolvers} that compute a solution iteratively.
    \item \textbf{Direct methods} \cite{davis2006directSPsolvers} that solve the system in just one step.
\end{enumerate}

\noindent The former ones like \textit{Conjugate Gradient} or \textit{Generalized Minimal Residual Method} - shortened as GMRES - exploit fast matrix-vector product to deliver good performances even if the solution is found iteratively. Those often use also \textit{preconditioning} that consist in the application of a transformation - called the preconditioner - that turns the system into a form more suitable for numerical solving methods - e.g. \textit{Preconditioned Conjugate Gradient} (PCG). The latter group, instead, exploits matrix decompositions like \textit{Cholesky} or the $QR$-decomposition to efficiently retrieve a solution in one step. Crucial for those kind of methods is the \textit{fill-in} - i.e. the increase of non-zero elements in the decomposition with respect to the source matrix. As already mentioned in Section \ref{sec:sparse_ls}, variable reordering techniques, that consists in applying a suitable permutation to the source matrix, are able to reduce dramatically the fill-in, allowing a faster decomposition. Several algorithm are proposed in the literature to compute the proper permutation - e.g. AMD, COLAMD. For the Cholesky $LU$ decomposition it is also possible to evaluate the pattern of the $L$ matrix before computing the actual decomposition, through the \textit{Symbolic Cholesky Decomposition}. 

In the remaining of the Section, it will be given a more detailed description on how to solve a sparse linear system using the Cholesky decomposition of the matrix $\hessian$.

\subsection{Storage Methods for Sparse Matrices}\label{subsec:sparse_storage_methods}
Another core aspect of sparse matrices is that there are several techniques to \textit{store} them more efficiently, reducing the amount of memory needed. The most general ones are \textit{Compressed Row Storage} (CRS) or \textit{Compressed Column Storage} (CCS), since they do not make any assumption on the structure of the matrix but the do not store unnecessary elements - i.e. the zeros. Those method store the matrix using only 3 vectors: one vector for floating point numbers that represent the non-zero entries and two for the column and row indexes respectively. As an example, given a matrix $A$

\begin{equation*}
    A = 
        \begin{pmatrix}
            10 & 0 & 0 & 0 & -2 & 0 \\
            3 & 9 & 0 & 0 & 0 & 3 \\
            0 & 7 & 8 & 7 & 0 & 0 \\
            3 & 0 & 8 & 7 & 5 & 0 \\
            0 & 8 & 0 & 9 & 9 & 13 \\
            0 & 4 & 0 & 0 & 2 & -1 
        \end{pmatrix}
\end{equation*}

\noindent in CRS format is represented by the following vectors - using zero-based indexing:

\begin{align*}
    val &= \begin{bmatrix}10 & -2 & 3 & 9 & 3 & 7 & \cdots & 2 & -1\end{bmatrix} \\
    col &= \begin{bmatrix}0 & 4 & 0 & 1 & 5 & 1 & \cdots & 4 & 5\end{bmatrix} \\
    row &= \begin{bmatrix}0 & 2 & 5 & 8 & 12 & 16 & 19\end{bmatrix}     
\end{align*}

\noindent \textit{List of Lists} (LIL) represents another effective and easy to implement storage method is the. In this case, each row-vector is represented as a list of pairs that denotes the column index and the element's value.

Recalling equation \ref{eq:sparse_hessian_structure}, since in this problem it has been considered only \textit{binary constraints}, the linearization of each edge generates $4$ block-contributions to the Hessian, namely:

\begin{align*}
    \hessian_{ii} &= \jacob_i^T \Omega_k \, \jacob_i \qquad
    \hessian_{jj} = \jacob_j^T \Omega_k \, \jacob_j \\
    \hessian_{ij} &= \jacob_i^T \Omega_k \, \jacob_j \qquad
    \hessian_{ji} = \jacob_j^T \Omega_k \, \jacob_i 
\end{align*}

\noindent Therefore, the Hessian can be intended to be a \textit{sparse block-matrix} where each entry is a matrix itself of a given size. In our work, it has been chosen to employ the LIL storage method with block-entries, in order to speed-up the numerical computations - better explained in the next Chapter.

\subsection{Cholesky Decomposition}\label{subsec:cholesky_dec_general}
In linear algebra, the Cholesky decomposition (or factorization) is the decomposition of a \textit{Hermitian}, positive semi-definite (PSD) matrix into the product of a \textit{lower triangular matrix} and its \textit{conjugate transpose}, namely

\begin{empheq}[box={\mybluebox[1pt]}]{equation}
    \label{eq:generic_cholesky}
    \mathbf{A} = \L\,\L^\star
\end{empheq}

\noindent In this particular problem formulation, the matrices involved are real, therefore the conjugate transpose of $\L$ is simply its transposed $\L^T = \U$. 

A more stable variant of the classical Cholesky decomposition is the \textbf{LDL} decomposition. In this case, the original matrix is decomposed into the following product:

\begin{equation}
    \label{eq:ldl_decomposition}
    \mathbf{A} = \L \mathbf{D} \L^\star
\end{equation}

\noindent where $\L$ is a lower \textit{unit} triangular matrix - i.e. all the entries on the main diagonal are $1$ - and $\mathbf{D}$ a diagonal matrix. This variant requires the same space and computational effort with respect to the original one but avoids the square-roots extraction. In this way, even matrices that do not have a Cholesky decomposition can be factorized with the \textit{LDL} one. However, in this work, since the matrix is symmetric and PSD by construction, it has been chose the original Cholesky decomposition.

In general the computational complexity for the factorization of a $(n \times n)$ matrix is $O(n^3)$, requiring about $\nicefrac{n^3}{3}$ FLOPs. There are several algorithm available to compute the factorization, however, one of the most common is the \textit{Cholesky-Banachiewicz}. In this algorithm the computation starts from the top-left corner of the matrix $\L$ and proceeds the computation row-by-row as follows:

\begin{equation*}
    \mathbf{A} = \L \L^T = 
        \begin{pmatrix}
            L_{00} & 0 & 0 \\ L_{10} & L_{11} & 0 \\ L_{20} & L_{12} & L_{22} 
        \end{pmatrix}
        \,
        \begin{pmatrix}
            L_{00} & L_{10} & L_{20} & \\ 0 & L_{11} & L_{21} \\ 0 & 0 & L_{22}
        \end{pmatrix}
\end{equation*}

\noindent where

\begin{empheq}[box={\mybluebox[3pt]}]{equation}
    \label{eq:cholesky_banachiewicz}
    \begin{cases}
        L_{jj} = \sqrt{A_{jj} - \sum_{k = 1}^{j-1}L_{j k}^2} \\
        L_{ij} = \frac{1}{L_{jj}}\left[A_{ij} - \sum_{k = 1}^{j-1}L_{ik}\,L_{jk}\right] \qquad\quad \text{for}\quad i>j
    \end{cases}
\end{empheq}

This algorithm and also the \textit{Cholesky-Crout}'s one - that proceeds column-by-column instead - allow to perform the computation also \textit{in-place}. Moreover, both algorithms can be employed in sparse block-matrices, leading to a block-Cholesky factorization. The blocks are also computed as in Equation \ref{eq:cholesky_banachiewicz} but, in the block case, the \textit{square root} operator is applied to the matrix-block and it consists in the \textit{Cholesky decomposition of the block} itself.

As it has been already mentioned, the main use of the Cholesky decomposition is in the solution of linear systems. Given a symmetric PSD real matrix $\mathbf{A}$, the solution of the linear system $\mathbf{A}\,\state = \bvec$ is computed through the following steps:

\begin{enumerate}
    \item \textbf{Cholesky factorization} of source matrix $\mathbf{A} = \L \L^T$
    \item \textbf{Forward substitution} to solve the linear system $\L \, \mathbf{y} = \bvec$
    \item \textbf{Backward substitution} to solve the linear system $\L^T\, \state = \mathbf{y}$
\end{enumerate}

Clearly, since in the problem in analysis $\hessian$ and its factorization $\L$ are sparse block-matrices, also $\bvec$ and $\mathbf{y}$ are \textit{dense block-vector} - with the number of blocks $N$ equal to the number of vertexes in the graph.

\section{Manifold Representation}\label{sec:manifold_se3}
As it has been already stated at the begin of the Chapter, this work focuses on 3D formulations of pose-graph and bundle adjustment. In this Section it will be better analyzed the representation of all the objects required for the LS estimation in both the formulations.

\subsection{3D Pose-Graph}\label{subsec:3d_pose_graphs}
Pose-graph optimization in 3D represents the backbone of SLAM, allowing to estimate the robot trajectory \textit{in space} through MAP estimation. In this formulation, the state $\state$ includes the 3D orientation of the nodes which represent the main reason why pose-SLAM is a complex problem. Rotations in the space can be over-parametrized through a 3D rotation matrix $\rot \in SO(3)$. Therefore, the over-parametrized state can be represented by a 3D isometry $\SState = \,\T{W}{R} \in SE(3)$ which represents the robot pose in the world reference frame - i.e. a $(4 \times 4)$ homogeneous transformation matrix.

A possible \textit{minimal representation} of the state can be through a $6$ vector $\state = (t_x\,t_y\,t_z\,\alpha\,\beta\,\gamma)^T$, where the triplet $\mathbf{r} = (\alpha\,\beta\,\gamma)^T$ represents the Euler Angles that compose the rotational part of the isometry, while $\trans = (t_x\,t_y\,t_z)^T$ the translational one. Summarizing, in formul\ae:

\begin{equation*}
    \SState = 
        \begin{pmatrix}
            \rot & \trans \\ \zero^T & 1
        \end{pmatrix}
    \qquad 
    \state = \begin{bmatrix} t_x & t_y & t_z & \alpha & \beta & \gamma \end{bmatrix}^T
\end{equation*}

\noindent where

\begin{equation}
    \label{eq:se3_rotation}
    \rot = \rot_x(\alpha) \, \rot_y(\beta)\, \rot_z(\gamma)
\end{equation}

The measurements are also of type \textit{pose}, thus, it is possible to use the same notation that describes the state. Therefore $\Meas_{ij}$ represents the over-parametrized measurement of node $j$ with respect to node $i$ - i.e. a 3D isometry $\T{i}{j}$.

The next required step concerns the definitions of suitable operators \textit{box-plus} and \textit{box-minus}. We introduce again the operators \textit{v2t} and \textit{t2v} that allow to map the over-parametrized representation into the minimal one and vice versa. Those two operators allow to define the following relations:

\begin{align}
    \label{eq:standard_boxplus_se3}
    \SState \boxplus \dx &= \v2t(\dx) \, \SState \\
    \label{eq:standard_boxminus_se3}
    \SState_a \boxminus \SState_b &= \text{t2v}\left(\SState_b^{-1}\SState_a\right)
\end{align}

\noindent For $SE(3)$ object, the \textit{v2t} function computes the rotational part of $\SState$ through Equation \ref{eq:se3_rotation} and then composes the isometry adding the translational part $\trans = (t_x \: t_y \: t_z)^T$. In Equation \ref{eq:se3_rotation} the factors $\rot_x$, $\rot_y$ and $\rot_z$ represent the 3D rotation respectively around the $x$, $y$ and $z$ axis; they are defined as follows:

\begin{align}
    \label{eq:rotx_3d}
    \rot_x(\alpha) &= 
        \begin{bmatrix}
            1 & 0 & 0 \\ 0 & \cos\alpha & -\sin\alpha \\ 0 & \sin\alpha & \cos\alpha
        \end{bmatrix} \\
    \label{eq:roty_3d}
    \rot_y(\beta) &=
        \begin{bmatrix}
            \cos\beta & 0 & \sin\beta \\ 0 & 1 & 0\\ -\sin\beta & 0 & \cos\beta
        \end{bmatrix} \\
    \label{eq:rotz_3d}
    \rot_z(\gamma) &= 
        \begin{bmatrix}
            \cos\gamma & -\sin\gamma & 0 \\ \sin\gamma & \cos\gamma & 0 \\ 0&0&1
        \end{bmatrix} 
\end{align}

Instead, to retrieve the Euler given the rotation matrix $\rot$ - that is done in the \textit{t2v} operator - it is necessary to equate each element in $\rot$ with its corresponding element in the matrix product $\rot_x(\alpha) \, \rot_y(\beta)\, \rot_z(\gamma)$, in formul\ae:

\begin{align}
    \label{eq:rot_composition_xyz}
    \rot &= 
        \begin{bmatrix}
            R_{00} & R_{01} & R_{02} \\
            R_{10} & R_{11} & R_{12} \\
            R_{20} & R_{21} & R_{22} 
        \end{bmatrix}
     = \rot_x(\alpha) \, \rot_y(\beta)\, \rot_z(\gamma) = \\
     &=
        \begin{bmatrix}
            \cos\beta\,\cos\gamma & -\cos\beta\,\sin\gamma & \sin\beta \\
            \cos\alpha\,\sin\gamma + \sin\alpha\,\cos\gamma\,\sin\beta & \cos\alpha\,\cos\gamma - \sin\alpha\,\sin\beta\,\sin\gamma & -\cos\beta\,\sin\alpha \\
            \sin\alpha\,\sin\gamma - \cos\alpha\,\cos\gamma\,\sin\beta & \sin\alpha\,\cos\gamma + \cos\alpha\,\sin\beta\,\sin\gamma & \cos\beta\,\cos\alpha 
        \end{bmatrix} \nonumber
\end{align}

The reader might notice the non-linearities introduced by the functions t2v and v2t. Those imply the computation of complex non-linear derivatives to retrieve the Jacobian in the linearization phase. However, this problem - and the solution proposed in this work - will be better analyzed in the next Section.

\subsection{3D Bundle Adjustment}\label{subsec:3d_bundle_adj}
In this formulation, as already explained in Section \ref{sec:bundle_adjustment_problem}, the system has to estimate both the robot trajectory and the position of salient world points - i.e. the 3D landmarks. Therefore, the system's \textit{state} and \textit{increment} are described as follows:

\begin{align*}
    \SState &= \left(\overbracket{\SState^R_1, ..., \SState^R_N}^{N poses}\: |\: \overbracket{\state^L_{N+1}, ..., \state^L_{N+M}}^{M landmarks}\right) \\
    \dx &= \left(\overbracket{\dx^R_1, ..., \dx^R_N}^{N poses}\: |\: \overbracket{\dx^L_{N+1}, ..., \dx^L_{N+M}}^{M landmarks}\right)
\end{align*}

The formalization for $SE(3)$ nodes remains unchanged from the previous Sub-Section, therefore, it is necessary to characterize only the nodes that describe the landmarks. 

Landmarks' nodes lie on $\mathbb{R}^3$, so it is not necessary to define anything else - i.e. no \textit{box-plus}/\textit{box-minus} operator needed. As a consequence of this, a measurement $\meas_{ij} \in \mathbb{R}^3$ is a simple 3 vector that describes the position of point $j$ in the $i$-th pose reference frame.

However, it is necessary to consider also the fact that the sensor's reference frame and the robot's one might not coincide, but they related through the transformation $\S = \,\T{R}{S} \in SE(3)$. Given this, the \textit{predicted measurement} given the state is:

\begin{equation}
    \label{eq:prediction_se3r3}
    \tmeas_{ij} = h_{ij}(\SState) = \underbracket{\S^{-1}\,\SState_i^{-1}}_{\mathbf{K}}\p_j = \rot_{K}\p_j + \trans_K
\end{equation}

\noindent In light of this, without loss of generality, the error between the predicted and the actual measurement is computed as follows:

\begin{empheq}[box={\mybluebox[0pt]}]{equation}
    \label{eq:error_se3r3}
    \error_{ij} = \tmeas_{ij} - \meas_{ij} = \S^{-1}\SState_i^{-1}\p_j - \meas_{ij}
\end{empheq}

\noindent Given Equation \ref{eq:error_se3r3}, the perturbed error will be:

\begin{equation}
    \label{eq:perturbed_error_se3r3}
    \error_{ij}(\SState_i \boxplus \dx_i^R, \state_j + \dx_j^L) = \S^{-1}\left[\left(\v2t(\dx_i^R)\SState_i\right)^{-1}\left(\p_j+\dx_j^L\right)\right] - \meas_{ij}
\end{equation}

Finally, it is necessary to compute the Jacobian $\jacob$ deriving from the constraint $\meas_{ij}$, that is structured as follows:

\begin{equation*}
    \jacob = \left[\zero \: \cdots \: \zero \:\, \jacob_R \,\: \zero \: \cdots \: \zero \:\, \jacob_L \,\: \zero \: \cdots \: \zero\right]
\end{equation*}

\noindent where

\begin{align*}
    \jacob_R &= \frac{\partial\,\S^{-1} \left[\left(\v2t(\dx_i^R)\SState_i\right)^{-1}\left(\p_j + \dx_j^L\right) - \meas_j\right]}{\partial\dx_i^R} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = \\
    &= \frac{\partial\: \left[\rot_{S}^T\left[\v2t(\dx_i^R) \SState_i\right]^{-1} \, \p_j - \rot_{S}^T\trans_S\right]}{\partial\dx_i^R} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = \\
    &= \frac{\partial\, \rot_{S}^T \SState_i^{-1} \left[\v2t(\dx_i^R)\right]^{-1} \, \p_j}{\partial\dx_i^R} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = \\
    &= \frac{\partial\, \left[\rot_{S}^T\, \rot_{R}^T\, \left[\v2t(\dx_i^R)\right]^{-1} \, \p_j - \rot_{R}^T\,\trans_R\right]}{\partial\dx_i^R}\Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = \\
    &= \frac{\partial\, \rot_{S}^T \, \rot_{R}^T \left(\left[\v2t(\dx_i^R)\right]^{-1} \, \p_j\right)}{\partial\dx_i^R} \Bigg \rvert_{\tiny\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} =
    \rot_{S}^T \, \rot_{R}^T \frac{\partial\, \left[\rot_{\dx_i^R} \,\p_j - \rot_{\dx_i^R} \,\trans_{\dx_i^R}\right]}{\partial\dx_i^R} \Bigg \rvert_{\tiny\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} 
\end{align*}

\noindent Exploiting the fact that the derivative expressed in the previous equation is evaluated in $\dx_i^R = 0$, it leads to the following relation:

\begin{empheq}[box={\mybluebox[0pt]}]{equation}
    \label{eq:jac_r_se3r3}
    \jacob_R = \rot_{S}^T \: \rot_{R}^T \: \begin{bmatrix} -I_{3\times3} & | & -{\left[\p_j\right]}_\times\end{bmatrix}
\end{empheq}

\noindent The other component of $\jacob$ - namely $\jacob_L$ - instead can be computed as follows:

\begin{align*}
    \jacob_L &= \frac{\partial\,\S^{-1} \left[\left(\v2t(\dx_i^R)\SState_i\right)^{-1}\left(\p_j + \dx_j^L\right) - \meas_j\right]}{\partial\dx_j^L} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = \\
    &= \frac{\partial\,\left[\rot_{S}^T \, \SState_i^{-1}\left(\p_j + \dx_j^L\right) - \rot_{S}^T \, \trans_S\right]}{\partial\dx_j^L} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = \\
    &= \frac{\partial\,\left[\left(\rot_{S}^T \, \SState_i^{-1}\p_j\right) +  \left(\rot_{S}^T \, \SState_i^{-1}\dx_j^L\right)\right]}{\partial\dx_j^L} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} = 
    \rot_{S}^T \, \frac{\partial\,\left[\rot_R^T \, \dx_j^L  - \rot_{R}^T\trans_R\right]}{\partial\dx_j^L} \Bigg \rvert_{\scriptsize\begin{matrix} \dx_i^R = 0\\\dx_j^L = 0\end{matrix}} 
\end{align*}

%\dx_i^R = 0, \dx_j^L = 0

\noindent This result - expanding the derivatives and exploiting that the linearization point is $\dx_j^L = 0$ - leads to the following relation:

\begin{empheq}[box={\mybluebox[0pt]}]{equation}
    \label{eq:jac_l_se3r3}
    \jacob_L = \rot_{S}^T\:\rot_{R}^T
\end{empheq}

\noindent The reader might notice that $\jacob_R$ is a $(3 \times 6)$ matrix - since the minimal representation of $SE(3)$ states has $6$ components - while $\jacob_L$ is a $(3\times3)$ matrix - because $\mathbb{R}^3$ states are vectors with only $3$ components. 

In the next Section it is proposed a deeper analysis on the error representation and the linearization of 3D pose constraints, highlighting the non-linearity of the computation and the proposed approach to overcome this issue.